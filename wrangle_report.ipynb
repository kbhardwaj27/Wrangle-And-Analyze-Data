{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in Wrangling\n",
    "- Gather\n",
    "- Assess\n",
    "- Clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather\n",
    "In this step we obtain data for each of the three different pieces of data in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced Twitter Archive\n",
    "    This file has been provided to us as a csv file.\n",
    "    \n",
    "1. We used read_csv function as below to load the file and create df<br />\n",
    "    twitter_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')<br />\n",
    "    twitter_archive_enhanced.sample()*Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data via the Twitter API\n",
    "    As the basic Twitter metrics: retweet count and favorite count were missing from Enhanced Twitter Archive Data, we'll query and gather this data through Twitter API\n",
    "1. Setup twitter connection<br />\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)<br />\n",
    "auth.set_access_token(access_token, access_secret)<br />\n",
    "api = tweepy.API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "2. Create list of tweet_ids from Enhanced Twitter Archive data.<br />\n",
    "tweet_id_list=twitter_archive_enhanced.tweet_id.to_list()\n",
    "3. Gather data from Twitter API.<br />\n",
    "tweets, drop_lst = [], []<br />\n",
    "start = time.time()<br />\n",
    "for id in tweet_id_list:<br />\n",
    "    try:<br />\n",
    "        tweet = api.get_status(id,tweet_mode='extended')<br />\n",
    "        tweets.append(tweet._json)\n",
    "        \n",
    "    except tweepy.TweepError as e:\n",
    "        error = e.args[0][0]['message']\n",
    "        drop_lst.append({'tweet_id':id,\n",
    "                        'error':error})\n",
    "    end = time.time()<br />\n",
    "    *To monitor total time taken for gathering data from Twitter API<br />*\n",
    "    print(end - start)<br />\n",
    "4. Write data to a text file<br />\n",
    "    with open('twee_json.txt', 'w') as fh:<br />\n",
    "    fh.write('\\n'.join(json.dumps(tweet) for tweet in tweets))\n",
    "5. Read data from text file and create dataframe<br />\n",
    "    with open(\"twee_json.txt\") as fh:<br />\n",
    "    tweets = [json.loads(line) for line in fh if line]<br />\n",
    "    twitter_api_data = pd.DataFrame(tweets)<br />\n",
    "    twitter_api_data.sample()\n",
    "6. Create dataframe of all tweet_ids for which twitter data wasn't found.<br />\n",
    "    t_api_unfound_data = pd.DataFrame(drop_lst,columns=['tweet_id','error'])<br />\n",
    "    t_api_unfound_data.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Predictions File\n",
    "    Data from running all the images through a neural network. \n",
    "    This data is hosted on Udacity's servers and needs to be downloaded programmatically using the Requests library. \n",
    "1. Downloading data and reading the .tsv file and creating dataframe<br />\n",
    "    url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'<br />\n",
    "    download = requests.get(url)<br />\n",
    "    with open(url.split('/')[-1], mode = 'wb') as outfile:<br />\n",
    "     outfile.write(download.content)\n",
    "\n",
    "    image_predictions = pd.read_csv('image-predictions.tsv', sep = '\\t', encoding = 'utf-8')<br />\n",
    "    image_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess\n",
    "This is the second step in data wrangling process. In this step we assess data for two types of issues:- <br />\n",
    "- Quality *issues with content.*\n",
    " - Examples:- incorrect datatypes, data outliers, etc. \n",
    "- Tidiness *issues with structure that prevent easy analysis.*\n",
    " - Examples:- Multple tables for one row, multiple columns. \n",
    " \n",
    "#### Enhanced Twitter Archive\n",
    "- twitter_archive_enhanced.shape[0] *To get columns,rows of the dataframe*\n",
    "- twitter_archive_enhanced.info() *To get quick overview of the dataframe*\n",
    "- twitter_archive_enhanced.sample() *To view a random row of the dataframe*\n",
    "- type(twitter_archive_enhanced['timestamp'][0]) *To get the data type of the column* __Incorrect datatype__\n",
    "- twitter_archive_enhanced.name.value_counts() *To view count associated with different Name values in dataframe* __Incorrect enteries such as 'a', 'an', 'the' & 'None'__\n",
    "- twitter_archive_enhanced.rating_denominator.value_counts() *To view count associated with different Denominator values in dataframe* __Per the description of the project Denominator Ratings should be 10 but here we have outliers__\n",
    "- twitter_archive_enhanced[twitter_archive_enhanced['retweeted_status_id'].notnull()].head() *To view rows in dataframe with not null values in Retweeted_Status_ID column* \n",
    "#### Image Predictions File\n",
    "- image_predictions.shape[0] *To get columns,rows of the dataframe*\n",
    "- image_predictions.sample() *To view a random row of the dataframe*\n",
    "- image_predictions.info() *To get quick overview of the dataframe*\n",
    "- image_predictions.describe() *To view basic statistical details such as percentile, mean, std etc. of a dataframe.*\n",
    "- image_predictions.describe(include='all') *To include unique, top, freq of string columns along with basic statistics of numeric columns.*\n",
    "- image_predictions.nunique() *To view unique rows in dataframe.*\n",
    "- image_predictions.head() *To view top 5 rows in dataframe.* __From this we can identify '_' in names of the breed precitions with two words is a quality issue and mixture of cases in the breed names, sometimes lowercase, etc is a quality issue too.__\n",
    "- sum(image_predictions.duplicated()) *To get count of duplicate rows in dataframe.*\n",
    "- sum(image_predictions.tweet_id.duplicated()) *To get count of duplicate rows in dataframe based on __tweet_id__ column.*\n",
    "- sum(image_predictions.jpg_url.duplicated()) *To get count of duplicate rows in dataframe based on __jpg_url__ column.* __We found 66 duplicate rows.__\n",
    "- duplicateRows_image_predictions = image_predictions[image_predictions.duplicated(['jpg_url'],keep=False)] *Create df with duplicate rows based on __jpg_url__ column.*\n",
    "- duplicateRows_image_predictions.sort_values(by='jpg_url').head(10) *View top 10 rows of duplicate rows to do viual assessment to identify any Quality issue.* __We found duplicate tweets of same data which could skew our anlysis. This is a quality issue.__\n",
    "#### Twitter API Data\n",
    "- twitter_api_data.shape[0] *To get columns,rows of the dataframe*\n",
    "- twitter_api_data.info() *To get quick overview of the dataframe*\n",
    "- twitter_api_data.sample() *To view a random row of the dataframe* __Downloaded data has toomany columns which we don't need for analysis i.e. a quality issue__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "##### `Enhanced Twitter Archive` table\n",
    "1. Erroneous datatype timestamp\n",
    "2. Remove Retweets based on Retweeted Status Id\n",
    "3. Drop columns retweeted_status_id,retweeted_status_user_id & retweeted_status_timestamp\n",
    "4. Name column has incorrect data.\n",
    "5. ratings_denominator have outliers. \n",
    "6. Reorganising dog_stages data into different buckets\n",
    "\n",
    "##### `Image Predictions` table\n",
    "7. '_' in the breed predicted columns. \n",
    "8. Unsymmetric casetypes in breed predicted columns.\n",
    "9. Remove duplicate tweets based on same jpg_url\n",
    "\n",
    "##### `Twitter API Data` table\n",
    "10. Drop unnecessary columns\n",
    "\n",
    "#### Tidiness\n",
    "1. One variable in four columns in `Enchanced Twitter Archive` table (dog_stage)\n",
    "2. retweet count and favorite count should be part of `Enchanced Twitter Archive`\n",
    "3. There should be one combined dataset for each tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "This is the third step in data wrangling process. In this step we fixed the __Quality__ & __Tidiness__ issues identified in __Assess__ step.\n",
    " \n",
    "### Quality\n",
    "##### Create copy of dataframes so initial dataframes remains unaffected.\n",
    "\n",
    "tae_clean = twitter_archive_enhanced.copy()<br />\n",
    "image_pred_clean = image_predictions.copy()<br />\n",
    "tad_clean = twitter_api_data.copy()<br />\n",
    "#### Enhanced Twitter Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Erroneous datatype timestamp\n",
    "*remove microseconds from timestamp and convert column timestamp to timestamp datatype*<br />\n",
    "tae_clean['timestamp'] =pd.to_datetime(tae_clean.timestamp, format=\"%Y-%m-%d %H:%M:%S +%f\")*Converting to timestamp datatype.<br /> \n",
    "tae_clean['timestamp'] = tae_clean.timestamp[:-5] *Removing microseconds*<br />\n",
    "tae_clean.sample() *Verify*<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Remove Retweets based on Retweeted Status Id\n",
    "tae_clean = tae_clean[tae_clean.retweeted_status_id.isnull()] *Recreating df from df excluding rows where rewteeted_status_id is Null as we want to exclude rewteeted ids* <br />\n",
    "tae_clean.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Drop columns retweeted_status_id,retweeted_status_user_id & retweeted_status_timestamp\n",
    "tae_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1,inplace=True) *Dropping columns retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp*<br />\n",
    "tae_clean.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. Name column has incorrect data.\n",
    "*create df with lowercase name values and uppercase name case values*\n",
    "tae_clean_lowercase_name = tae_clean[tae_clean['name'].str[0].str.islower()]<br />\n",
    "tae_clean_uppercase_name = tae_clean[tae_clean['name'].str[0].str.isupper()]<br />\n",
    "\n",
    "*View value count of dfs based on Name column.*\n",
    "    tae_clean_lowercase_name.name.value_counts()<br />\n",
    "    tae_clean_uppercase_name.name.value_counts()<br />\n",
    "*__As we can visually assess, lowercase Name values are all incorrect.__*<br />\n",
    "    \n",
    "*Remove Lowercase values from Name column*<br />\n",
    "    tae_clean = tae_clean[~tae_clean['name'].str[0].str.islower()]<br />\n",
    "    tae_clean.name.value_counts() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. ratings_denominator have outliers. \n",
    "*Drop rows which have rating_denominator not equal to 10.*<br />\n",
    "tae_clean = tae_clean[tae_clean['rating_denominator']==10] *Removing rows which have ratings_denominator __Not Equal__ to __10__.*<br />\n",
    "tae_clean.rating_denominator.value_counts() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Predictions File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. '_' in the breed predicted columns. \n",
    "*Replace \" _ \" from __p1, p2, p3__ coulumn values.*<br />\n",
    "image_pred_clean['p1'] = image_pred_clean['p1'].str.replace('_ ', ' ') <br />\n",
    "image_pred_clean['p2'] = image_pred_clean['p2'].str.replace('_ ', ' ')<br />\n",
    "image_pred_clean['p3'] = image_pred_clean['p3'].str.replace('_ ', ' ')<br />\n",
    "\n",
    "image_pred_clean.head() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Unsymmetric casetypes in breed predicted columns.\n",
    "*Fix random case for values of __p1, p2, p3__ coulumns to Title Case.*<br />\n",
    "image_pred_clean['p1'] = image_pred_clean['p1'].str.title()<br />\n",
    "image_pred_clean['p2'] = image_pred_clean['p2'].str.title() <br />\n",
    "image_pred_clean['p3'] = image_pred_clean['p3'].str.title() <br />\n",
    "\n",
    "image_pred_clean.head() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9. Remove duplicate tweets based on same jpg_url.\n",
    "image_pred_clean[image_pred_clean.duplicated(['jpg_url'],keep=False)].sort_values(by='jpg_url').head(6) <br />\n",
    "__Based on above output we can visually assess duplicates__\n",
    "\n",
    "*Drop duplicate rows*<br />\n",
    "image_pred_clean.drop_duplicates(['jpg_url'],inplace=True)<br />\n",
    "image_pred_clean[image_pred_clean.duplicated(['jpg_url'],keep=False)].sort_values(by='jpg_url').head(6) *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter API Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Drop unnecessary columns\n",
    "*Create column with tweet_id, favorite_count & retweet_count. \n",
    "We need Tweet_id to join the dataframe with Enhanced Twitter Archive and we need favorite count and retweet count as they're unavailable in Enhanced Twitter Archive dataframe.*\n",
    "\n",
    "tad_clean = tad_clean.filter(['id','favorite_count','retweet_count'],axis=1)<br />\n",
    "tad_clean.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. One variable in four columns in `Enchanced Twitter Archive` table (dog_stage)\n",
    "*Melt the doggo, floofer, pupper, puppo columns to a dog_stage column.*\n",
    "\n",
    "*Replace None with '' in puppo column*<br />\n",
    "tae_clean['puppo'].replace('None','',inplace=True)<br />\n",
    "tae_clean.puppo.value_counts()\n",
    "\n",
    "*Replace None with '' in doggo column*<br />\n",
    "tae_clean['doggo'].replace('None','',inplace=True)<br />\n",
    "tae_clean.doggo.value_counts()\n",
    "\n",
    "*Replace None with '' in pupper column*<br />\n",
    "tae_clean['pupper'].replace('None','',inplace=True)<br />\n",
    "tae_clean.doggo.value_counts()\n",
    "\n",
    "*Replace None with '' in floofer column*<br />\n",
    "tae_clean['floofer'].replace('None','',inplace=True)<br />\n",
    "tae_clean.doggo.value_counts()\n",
    "\n",
    "*Melt*<br />\n",
    "tae_clean['dog_stages'] = tae_clean['doggo']+tae_clean['floofer']+tae_clean['pupper']+tae_clean['puppo'] <br />\n",
    "tae_clean.dog_stages.value_counts() *Verify*\n",
    "\n",
    "#### Quality \n",
    "###### 6. Reorganising dog_stages data into different buckets\n",
    "Based on **The Dogtionary** puppo, pupper & doggo are stages and floofer can be accredited with any dog with fur. \n",
    "Hence, it makes sense to make following replacements:- \n",
    "- doggopupper with doggo who's just behaving like a  pupper\n",
    "- doggopuppo with puppo as it's advanced stage of doggo\n",
    "- doggofloofer with doggo \n",
    "- floofer with none as it is not a stage, just any dog really. \n",
    "\n",
    "tae_clean['dog_stages'].replace('doggopupper','doggo',inplace=True)<br />\n",
    "tae_clean['dog_stages'].replace('doggofloofer','doggo',inplace=True)<br />\n",
    "tae_clean['dog_stages'].replace('doggopuppo','puppo',inplace=True)<br />\n",
    "tae_clean['dog_stages'].replace('floofer','',inplace=True)<br />\n",
    "tae_clean.dog_stages.value_counts() *Verify*\n",
    "\n",
    "*Replace '' with NaN in dog_stages column for cleaner data storage.*<br />\n",
    "tae_clean['dog_stages'].replace('',np.nan,inplace=True)<br />\n",
    "tae_clean.dog_stages.value_counts() *Verify*\n",
    "\n",
    "*Drop columns doggo, floofer, pupper, puppo*<br />\n",
    "tae_clean.drop(['doggo','floofer','pupper','puppo'],axis=1,inplace=True)<br />\n",
    "tae_clean.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness\n",
    "###### 2. retweet count and favorite count should be part of `Enchanced Twitter Archive`\n",
    "*Merge retweet count and favorite count from __`Twitter API Data`__ with __`Enchanced Twitter Archive`__*\n",
    "\n",
    "*Rename column id to tweet_id in Twitter API Data copy*<br />\n",
    "tad_clean.rename(columns = {'id':'tweet_id'},inplace=True)<br />\n",
    "tad_clean.info()\n",
    "\n",
    "*Merging Twitter API data with Enhanced Twitter Archive data.*<br />\n",
    "tae_clean = pd.merge(tae_clean,tad_clean, on='tweet_id',how='left')<br />\n",
    "tae_clean.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. There should be one combined dataset for each tweet_id\n",
    "*Merging Enhanced Twitter Archive data with Image Predictions data. \n",
    "We'll be doing a right join between Enhanced Twitter Archive data & Image Predictions data to anlyze only those tweets for which we have image predictions data.*\n",
    "\n",
    "tweet_data = pd.merge(tae_clean,image_pred_clean, on='tweet_id',how='right') *Creating holistic dataframe*\n",
    "\n",
    "tweet_data.info() *Verify*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store\n",
    "In this step we store the cleaned data.\n",
    "\n",
    "tweet_data.to_csv(\"twitter_archive_master.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
